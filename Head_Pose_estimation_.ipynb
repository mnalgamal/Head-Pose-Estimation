{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UBOpofo0YWy"
      },
      "source": [
        "##Install Mediapipe for face mesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e9lTcJ1BVjL9"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1eh6avc1QEo"
      },
      "source": [
        "##Import libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Wk1MpXCPVowt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2,math,glob\n",
        "import scipy.io as sio\n",
        "from math import cos, sin\n",
        "import mediapipe\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab.patches import cv2_imshow\n",
        "#Libaries for model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6cNKf7c0SWd"
      },
      "source": [
        "##Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SeGMaMt8VwDO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/Database/AFLW2000-3D.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LtkoG7XVV-eY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip /content/AFLW2000-3D.zip "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUgTduAo1a2b"
      },
      "source": [
        "##Read the images & extract the features from the face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0nvfrCRRWVt_"
      },
      "outputs": [],
      "source": [
        "images_dir = glob.glob('AFLW2000/*.jpg')\n",
        "# Empty list to store the image arrays\n",
        "features_data = []\n",
        "labels_data = []\n",
        "faceModule = mediapipe.solutions.face_mesh\n",
        "with faceModule.FaceMesh(static_image_mode=True) as faces:\n",
        "    # Loop over all the images in the directory\n",
        "    for img in images_dir:\n",
        "        # Read the image and corresponding label \n",
        "        imgs = img.split(\".\")\n",
        "        image = cv2.imread(imgs[0]+'.jpg')\n",
        "        label = sio.loadmat(imgs[0]+'.mat')['Pose_Para'][0][:3]\n",
        "         # processing the face to extract the landmark points (468 point) for each x,y\n",
        "        results = faces.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        if results.multi_face_landmarks != None: \n",
        "          # looping over the faces in the image\n",
        "          for face in results.multi_face_landmarks:\n",
        "              x_points = []\n",
        "              y_points = []\n",
        "              for landmark in face.landmark:\n",
        "                  x = landmark.x\n",
        "                  y = landmark.y\n",
        "                  x_points.append(x)\n",
        "                  y_points.append(y)\n",
        "              x_point = np.array(x_points)\n",
        "              y_point = np.array(y_points)\n",
        "              x_center = x_point - x_point[0]\n",
        "              y_center = y_point - y_point[0]   \n",
        "              features_data.append(np.hstack([x_center,y_center]))\n",
        "          # Convert the list to a NumPy array\n",
        "          features = np.array(features_data)\n",
        "          labels_data.append(label)\n",
        "          labels = np.array(labels_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zhntGjpNgNa2"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "y_train_pitch= y_train[:,0]\n",
        "y_train_yaw= y_train[:,1]\n",
        "y_train_roll= y_train[:,2]\n",
        "\n",
        "y_test_pitch= y_test[:,0]\n",
        "y_test_yaw= y_test[:,1]\n",
        "y_test_roll= y_test[:,2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mgjjETHXWLam"
      },
      "outputs": [],
      "source": [
        "# Create a SVR model\n",
        "pitch_model = SVR()\n",
        "yaw_model = SVR()\n",
        "roll_model = SVR()\n",
        "\n",
        "# Train the model on the training set\n",
        "pitch_model.fit(X_train, y_train_pitch)\n",
        "yaw_model.fit(X_train, y_train_yaw)\n",
        "roll_model.fit(X_train, y_train_roll)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_pitch = pitch_model.predict(X_test)\n",
        "y_pred_yaw = yaw_model.predict(X_test)\n",
        "y_pred_roll = roll_model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1eOCkbEgg38P"
      },
      "outputs": [],
      "source": [
        "score = cross_validate(pitch_model,features,labels[:,0],cv=5,scoring=[\"neg_mean_absolute_error\",'r2'])\n",
        "\n",
        "score = cross_validate(yaw_model,features,labels[:,1],cv=5,scoring=[\"neg_mean_absolute_error\",'r2'])\n",
        "\n",
        "score = cross_validate(roll_model,features,labels[:,2],cv=5,scoring=[\"neg_mean_absolute_error\",'r2'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "smSf8Y5LFGba"
      },
      "outputs": [],
      "source": [
        "#function to draw the pitch,yaw and roll \n",
        "def draw_axis(img, pitch,yaw,roll, tdx=None, tdy=None, size = 100):\n",
        "\n",
        "    yaw = -yaw\n",
        "    if tdx != None and tdy != None:\n",
        "        tdx = tdx\n",
        "        tdy = tdy\n",
        "    else:\n",
        "        height, width = img.shape[:2]\n",
        "        tdx = width / 2\n",
        "        tdy = height / 2\n",
        "\n",
        "    # X-Axis pointing to right. drawn in red\n",
        "    x1 = size * (cos(yaw) * cos(roll)) + tdx\n",
        "    y1 = size * (cos(pitch) * sin(roll) + cos(roll) * sin(pitch) * sin(yaw)) + tdy\n",
        "\n",
        "    # Y-Axis | drawn in green\n",
        "    #        v\n",
        "    x2 = size * (-cos(yaw) * sin(roll)) + tdx\n",
        "    y2 = size * (cos(pitch) * cos(roll) - sin(pitch) * sin(yaw) * sin(roll)) + tdy\n",
        "\n",
        "    # Z-Axis (out of the screen) drawn in blue\n",
        "    x3 = size * (sin(yaw)) + tdx\n",
        "    y3 = size * (-cos(yaw) * sin(pitch)) + tdy\n",
        "\n",
        "    cv2.line(img, (int(tdx), int(tdy)), (int(x1),int(y1)),(0,0,255),3)\n",
        "    cv2.line(img, (int(tdx), int(tdy)), (int(x2),int(y2)),(0,255,0),3)\n",
        "    cv2.line(img, (int(tdx), int(tdy)), (int(x3),int(y3)),(255,0,0),2)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "o-OevAAJF5yA"
      },
      "outputs": [],
      "source": [
        "#frames = ['/content/CPBannerFacemobile2x.jpg','/content/face-category-pathing-switcher.jpg','/content/photo-1597223557154-721c1cecc4b0.jfif']\n",
        "def draw_images(frames):\n",
        "  features_data_test = []\n",
        "  images = []\n",
        "  faceModule = mediapipe.solutions.face_mesh\n",
        "  with faceModule.FaceMesh(static_image_mode=True) as faces:\n",
        "    for img in frames:\n",
        "      try:\n",
        "         image = cv2.imread(img)\n",
        "      except:\n",
        "        image = img\n",
        "      # processing the face to extract the landmark points (468 point) for each x,y\n",
        "      results = faces.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "      if results.multi_face_landmarks != None: \n",
        "        # looping over the faces in the image\n",
        "        for face in results.multi_face_landmarks:\n",
        "            x_points = []\n",
        "            y_points = []\n",
        "            for landmark in face.landmark:\n",
        "                x = landmark.x\n",
        "                y = landmark.y\n",
        "                # note: the x and y values are scaled to the their width and height so we will get back their actual value in the image\n",
        "                shape = image.shape \n",
        "                x_points.append(x)\n",
        "                y_points.append(y)\n",
        "            x_point = np.array(x_points)\n",
        "            y_point = np.array(y_points)\n",
        "            x_center = x_point - x_point[0]\n",
        "            y_center = y_point - y_point[0]   \n",
        "            features =np.hstack([x_center,y_center]).reshape(1,-1)\n",
        "            # Convert the list to a NumPy array\n",
        "            y_pred_pitch = pitch_model.predict(features)\n",
        "            y_pred_yaw = yaw_model.predict(features)\n",
        "            y_pred_roll = roll_model.predict(features)\n",
        "            draw_axis(image,y_pred_pitch,y_pred_yaw,y_pred_roll,x_points[1]*shape[1],y_points[1]*shape[0])\n",
        "            images.append(image)\n",
        "  return images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OiTHgdp21FR"
      },
      "source": [
        "## Testing the model on the image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TBG9iDajGnMn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7475SMHD3AmQ"
      },
      "source": [
        "## Testing the model on a video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zLjIxMjeJGAn"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture('/content/WIN_20230221_16_12_20_Pro.mp4')\n",
        "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter('/content/output.mp4', # output video path\n",
        "\t\t\t\t\t\tfourcc, frame_rate, (width, height))\n",
        "frames =[]\n",
        "while True:\n",
        "        try:\n",
        "        # Read the next frame from the video\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "              break\n",
        "            frame = cv2.flip(frame,1)\n",
        "            frames.append(frame)\n",
        "        except Exception as error:\n",
        "         print(error)\n",
        "        \n",
        "for frame in draw_images(frames):\n",
        "    out.write(frame)         \n",
        "  \n",
        "cap.release()\n",
        "out.release()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}